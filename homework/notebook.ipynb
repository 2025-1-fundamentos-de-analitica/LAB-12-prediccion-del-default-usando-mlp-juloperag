{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d340076",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importar librerias\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import gzip\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import  precision_score, balanced_accuracy_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93c51e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funciones\n",
    "def load_data():\n",
    "    # Define las columnas categóricas\n",
    "    global categorical_features\n",
    "    categorical_features = ['SEX', 'EDUCATION', 'MARRIAGE'] \n",
    "    global numeric_features \n",
    "    numeric_features = ['LIMIT_BAL', 'AGE', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6',\n",
    "        'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6',\n",
    "        'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']\n",
    "    \n",
    "    train = pd.read_csv(\n",
    "        \"../files/input/train_data.csv.zip\",\n",
    "        index_col=False,\n",
    "        compression=\"zip\",\n",
    "    )\n",
    "    test = pd.read_csv(\n",
    "        \"../files/input/test_data.csv.zip\",\n",
    "        index_col=False,\n",
    "        compression=\"zip\",\n",
    "    )\n",
    "    return train, test\n",
    "\n",
    "def clear_data(df):\n",
    "    #Renombrar\n",
    "    df.rename(columns={\"default payment next month\": \"default\"}, inplace=True)\n",
    "    #Eliminacion de columna\n",
    "    df = df.drop(\"ID\", axis=1)\n",
    "    #Eliminacion elementos nulos \n",
    "    df.dropna(inplace=True)\n",
    "    #Cambia valores de educacion mayores a 4\n",
    "    df[\"EDUCATION\"] = df[\"EDUCATION\"].apply(lambda x: x if x<=4 else 4)\n",
    "    return df\n",
    "\n",
    "def make_train_test_split(df):\n",
    "    #Division en etiquetas \n",
    "    y_df =  df[\"default\"]\n",
    "    #Division en caracteristicas de entrada\n",
    "    x_df = df.drop(\"default\", axis=1)\n",
    "    return x_df, y_df\n",
    "\n",
    "\n",
    "def cross_validation(pipeline, param_grid, x_train, y_train):\n",
    "    #Evaluacion de hiperparametros\n",
    "    model = GridSearchCV(\n",
    "        estimator = pipeline,\n",
    "        param_grid = param_grid,\n",
    "        cv = 10,\n",
    "        scoring=\"balanced_accuracy\", #\"f1_score\"\n",
    "    )\n",
    "    #Aplicacion de GridSearchCV\n",
    "    model.fit(x_train, y_train)\n",
    "    return model\n",
    "\n",
    "def save_grid_search_model(model):\n",
    "    #Guardar mejor modelo\n",
    "    if not os.path.exists(\"../files/models\"):\n",
    "        os.makedirs(\"../files/models\")\n",
    "    with gzip.open(\"../files/models/model.pkl.gz\", \"wb\") as file:\n",
    "        pickle.dump(model, file)\n",
    "\n",
    "def eval_metrics(type_dataset, y_true, y_pred):\n",
    "    #         | Pronóstico\n",
    "    #         |  PP    PN\n",
    "    #---------|------------\n",
    "    #      P  |  TP    FN\n",
    "    # Real    |\n",
    "    #      N  |  FP    TN\n",
    "\n",
    "    #(1/2)*(TP/P + TN/N)\n",
    "    b_accuracy = balanced_accuracy_score(y_true=y_true, y_pred=y_pred,)\n",
    "    #TP/(TP + FP)\n",
    "    precision = precision_score(\n",
    "        y_true=y_true,\n",
    "        y_pred=y_pred, \n",
    "        labels=None, \n",
    "        pos_label=1,\n",
    "        average=\"binary\",)\n",
    "    #TP/(TP + FN)\n",
    "    recall = recall_score(\n",
    "        y_true=y_true,\n",
    "        y_pred=y_pred, \n",
    "        labels=None, \n",
    "        pos_label=1,\n",
    "        average=\"binary\",)\n",
    "    #2*(precision*recall)/(precision + recall)\n",
    "    f1 = f1_score(\n",
    "        y_true=y_true,\n",
    "        y_pred=y_pred,\n",
    "        labels=None,\n",
    "        pos_label=1,\n",
    "        average=\"binary\",\n",
    "        sample_weight=None,\n",
    "        zero_division=\"warn\",)\n",
    "\n",
    "    #Formar diccionario de metricas \n",
    "    dic_metrics = { \"type\": \"metrics\",\n",
    "                   'dataset': type_dataset, \n",
    "                   'precision': precision , \n",
    "                   'balanced_accuracy': b_accuracy, \n",
    "                   'recall': recall, \n",
    "                   'f1_score': f1}\n",
    "    print(dic_metrics)\n",
    "    #Guardar metricas como archivo json\n",
    "    if not os.path.exists(\"../files/output\"):\n",
    "        os.makedirs(\"../files/output\")\n",
    "    with open(\"../files/output/metrics.json\", \"a\") as f:\n",
    "        json.dump(dic_metrics, f)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "def eval_confusion_matrix(type_dataset, y_true, y_pred):\n",
    "    #         | Pronóstico\n",
    "    #         |  PP    PN\n",
    "    #---------|------------\n",
    "    #      P  |  TP    FN\n",
    "    # Real    |\n",
    "    #      N  |  FP    TN\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true=y_true, y_pred=y_pred,).ravel()\n",
    "\n",
    "    #Formar diccionario de metricas \n",
    "    dic_confusion = {'type': 'cm_matrix', 'dataset': type_dataset, \n",
    "                   'true_0': {\"predicted_0\": int(tn), \"predicte_1\": int(fp)}, \n",
    "                   'true_1': {\"predicted_0\": int(fn), \"predicted_1\": int(tp)}}\n",
    "    print(dic_confusion)\n",
    "    #Guardar metricas como archivo json\n",
    "    if not os.path.exists(\"../files/output\"):\n",
    "        os.makedirs(\"../files/output\")\n",
    "    with open(\"../files/output/metrics.json\", \"a\") as f:\n",
    "        json.dump(dic_confusion, f)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "#---------------------------FUCTION SET------------------------------------------\n",
    "def dataset_manipulation():\n",
    "    #Carga de datos\n",
    "    train, test = load_data()\n",
    "    #Limpieza de datos\n",
    "    train = clear_data(train)\n",
    "    test = clear_data(test)\n",
    "    #Division en etiquetas y caracteristicas de entrada\n",
    "    x_train, y_train = make_train_test_split(train)\n",
    "    x_test, y_test = make_train_test_split(test)\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "def eval_model(model, x_train, y_train, x_test, y_test):\n",
    "    if os.path.exists(\"../files/output/metrics.json\"):\n",
    "        os.remove(\"../files/output/metrics.json\")\n",
    "    # Calculo de métricas\n",
    "    eval_metrics(\"train\", y_train, y_pred=model.best_estimator_.predict(x_train))\n",
    "    eval_metrics(\"test\", y_test, y_pred=model.best_estimator_.predict(x_test))\n",
    "    # Calculo matriz de confusión\n",
    "    eval_confusion_matrix(\"train\", y_train, y_pred=model.best_estimator_.predict(x_train))\n",
    "    eval_confusion_matrix(\"test\", y_test, y_pred=model.best_estimator_.predict(x_test))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7b5967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 3.\n",
    "# Cree un pipeline para el modelo de clasificación. Este pipeline debe\n",
    "# contener las siguientes capas:\n",
    "# - Transforma las variables categoricas usando el método\n",
    "#   one-hot-encoding.\n",
    "# - Descompone la matriz de entrada usando componentes principales.\n",
    "#   El pca usa todas las componentes.\n",
    "# - Escala la matriz de entrada al intervalo [0, 1].\n",
    "# - Selecciona las K columnas mas relevantes de la matrix de entrada.\n",
    "# - Ajusta una red neuronal tipo MLP.\n",
    "\n",
    "#------------------------------MODEL------------------------------------------\n",
    "def train_model(x_train, y_train):  \n",
    "    #----------------------PIPELINE------------------------------\n",
    "    # Crea el preprocesador\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            #Ej. df[Sex]-->\"1\",\"2\" por tanto la codificacion de esa columna sera un array de bit\n",
    "            # \"1\"-->[1,0] \"2\"-->[0,1]\n",
    "            ('one', OneHotEncoder(handle_unknown='ignore'), categorical_features),\n",
    "        ],\n",
    "        remainder='passthrough'  # Deja las columnas numéricas igual\n",
    "    )\n",
    "    #Contruccion pipeline\n",
    "    pipeline = make_pipeline(\n",
    "    preprocessor,\n",
    "    PCA(n_components=None), #Transformar los datos a un nuevo espacio ortogonal, eliminando correlaciones entre variables.\n",
    "    MinMaxScaler(feature_range=(0, 1)),\n",
    "    SelectKBest(k=15),\n",
    "    MLPClassifier(max_iter=10000),\n",
    "    )\n",
    "    #-------------------------PARAMETROS GRID-----------------------------\n",
    "    #Definicion de hiperparametros a evualuar \n",
    "    param_grid = {\n",
    "        \"selectkbest__k\": range(1, 11),\n",
    "        \"mlpclassifier__hidden_layer_sizes\": [(h,) for h in range(5, 10)],\n",
    "        \"mlpclassifier__learning_rate_init\": [0.0001, 0.001], #[0.0001, 0.001, 0.01, 0.1, 1.0],\n",
    "    }\n",
    "    #----------------------TRAIN CROSS-----------------------\n",
    "    model = cross_validation(pipeline=pipeline,\n",
    "                             param_grid=param_grid,\n",
    "                             x_train=x_train,\n",
    "                             y_train=y_train\n",
    "                             )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60954e0e",
   "metadata": {},
   "source": [
    "Ejecución del flujo principal para el dataset predefinido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a0b41c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carga y manipulacion de datos \n",
    "x_train, y_train, x_test, y_test = dataset_manipulation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74421774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "default\n",
       "0    16273\n",
       "1     4727\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Al verificar la cantidad de elementos de cada clase presentes en el dataset, \n",
    "#se determina que hay un desbalance entre las mismas, por tanto toca aplicar balanceo de la misma.\n",
    "#Clase 0: 16,273 (~77%)\n",
    "#Clase 1: 4,727 (~23%)\n",
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d38484",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b:\\Documentos\\Universidad-Materias\\Fundamentos_Analítica\\LAB-12-prediccion-del-default-usando-mlp-juloperag\\.venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:787: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "b:\\Documentos\\Universidad-Materias\\Fundamentos_Analítica\\LAB-12-prediccion-del-default-usando-mlp-juloperag\\.venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:787: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    }
   ],
   "source": [
    "#Definicion y entrenamiento de modelo\n",
    "model = train_model(x_train, y_train)  \n",
    "#Informacion del mejor modelo y ademas definirlo\n",
    "print(model.best_score_)\n",
    "print(model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c0b61ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Salvar mejor model\n",
    "save_grid_search_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6bf90d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'metrics', 'dataset': 'train', 'precision': 0.6887778282598819, 'balanced_accuracy': 0.6393082718312573, 'recall': 0.32071081023905224, 'f1_score': 0.4376443418013857}\n",
      "{'type': 'metrics', 'dataset': 'test', 'precision': 0.7, 'balanced_accuracy': 0.654079064505956, 'recall': 0.34834992142482973, 'f1_score': 0.4651976215459951}\n",
      "{'type': 'cm_matrix', 'dataset': 'train', 'true_0': {'predicted_0': 15588, 'predicte_1': 685}, 'true_1': {'predicted_0': 3211, 'predicted_1': 1516}}\n",
      "{'type': 'cm_matrix', 'dataset': 'test', 'true_0': {'predicted_0': 6806, 'predicte_1': 285}, 'true_1': {'predicted_0': 1244, 'predicted_1': 665}}\n"
     ]
    }
   ],
   "source": [
    "#Evaluacion del Modelo con diversas metricas\n",
    "eval_model(model, x_train, y_train, x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
